{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Flatline Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flatline.interpreter import Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new local interpreter, that will use *nodejs* under the rug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = Interpreter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available functions\n",
    "\n",
    "We can query the interpreter for all the built-in functions provided by flatline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.defined_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking symbolic expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpreter can check for us whether a Lisp or JSON s-expression is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid constant expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lisp s-expressions are represented as strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_lisp('(+ 1 2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON expressions are represented as Python lists of native values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_json([\"+\", [\"*\", 3, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some erroneous symbolic expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_lisp('(+ 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_json([\"non-existent\", 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_json([\"+\", 1, \"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_lisp('(f 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking expressions that depend on input dataset fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest sexp was invalid because no dataset is known, and hence there's no \"field 0\".\n",
    "\n",
    "Let's create a mock dataset to tell the interpreter what are our fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_dataset = {'dataset':{'fields': Interpreter.infer_fields([1, 'a'])}}\n",
    "mock_dataset['dataset']['fields']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checks referring to those fields will pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_lisp('(field 0)', dataset=mock_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_json([\"f\", \"000001\"], dataset=mock_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the two last expressions have no associated value, because they depend on the concrete input rows to which they're applied (i.e., these expressions do not represent constant values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying symbolic expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply valid symbolic expressions to local rows represented as lists of native Python values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows = [[1, 'a'], [2, 'b'], [23, 'd']]\n",
    "interpreter.apply_lisp('(fields 1 0)', test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.apply_lisp('(list (+ 2 (f 0)) (- (f 0) (f 0 -1)))', test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.apply_json([\"window\", \"000001\", -1, 1], test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, the field characteristics are guessed from the given values.  Guessing is useful for quick tests, but in real cases we should provide real dataset metadata to the apply functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended example using remote resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigml.api import BigML\n",
    "from flatline.sampler import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = BigML()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a dataset from Quandl's dataset on Apple NASDAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = api.create_source('https://s3.amazonaws.com/bigml-public/csv/nasdaq_aapl.csv', {'name':'Flatline tests'})\n",
    "api.ok(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = api.create_dataset(source)\n",
    "dataset_id = dataset['resource']\n",
    "api.ok(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And download a sample of its rows locally, using a *Sampler* object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sampler*, like *Interpreter* are abstractions above the building blocks provided by the API bindings, and take care internally of waiting for resource completion and other housekeeping (that's why we don't need `api.ok()` calls here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.take_sample(dataset_id, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the rows that we have downloaded locally (plus all the associated metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampler also keeps information on the dataset and sample metadata; e.g. the field descriptors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'id':f['id'], 'name':f['name'], 'optype':f['optype']} for f in sampler.fields()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply locally Flatline expressions and check whether they produce sensible results.  \n",
    "\n",
    "For instance, we could normalize **Low**, **High** and **Volume**, dividing them by their mean value in the original dataset.  \n",
    "\n",
    "Let's define an auxiliary function to generate the corresponding Flatline JSON s-expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_field(name):\n",
    "    return [\"/\", [\"field\", name], [\"abs\", [\"mean\", name]]]\n",
    "\n",
    "norm_field('High')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the interpreter to check the format and syntax of our generated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_as_lisp(json_sexp):\n",
    "    print interpreter.json_to_lisp(json_sexp)\n",
    "    \n",
    "print_as_lisp(norm_field('Low'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate more than one value, we wrap the list of field expressions in a `list` form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list(*fields):\n",
    "    res = ['list']\n",
    "    res.extend(fields)\n",
    "    return res\n",
    "    \n",
    "norm_fields = make_list(norm_field('Low'), norm_field('High'), norm_field('Volume'))\n",
    "print_as_lisp(norm_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check that the syntax is in fact correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_json(norm_fields, dataset['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our lisp expression seems correct, and produces three numeric values.  We can apply it to our sample rows and confirm that the outputs are in fact what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.apply_json(norm_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far.  Let's say we want to predict whether the stock will go up or down based on the Open and Close values of the **previous day** and today's Open value.  We can access the value of a previous row with `(field name -1)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_day(name):\n",
    "    return [\"field\", name, -1]\n",
    "\n",
    "open_close_fields = make_list(previous_day('Open'), \n",
    "                              previous_day('Close'))\n",
    "\n",
    "print_as_lisp(open_close_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it's a good Flatline expression and see how it works on our local sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.check_json(open_close_fields, dataset=dataset['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.apply_json(open_close_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the entries for the previous day Open and Close values are `None` in the first row, since there's no previous day!\n",
    "\n",
    "Finally, let's define our objective field, **UpOrDown**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_or_down = '(if (> (f \"Open\") (f \"Close\")) \"down\" \"up\")'\n",
    "interpreter.check_lisp(up_or_down, dataset=dataset['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.apply_lisp(up_or_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're happy with our transformations, we ask BigML to create the new fields over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fields_sexp = interpreter.json_to_lisp(norm_fields)\n",
    "open_close_sexp = interpreter.json_to_lisp(open_close_fields)\n",
    "\n",
    "extended_dataset = api.create_dataset(dataset, {'new_fields':[{'field':norm_fields_sexp, 'names':['NLow', 'NHigh', 'NVol']},\n",
    "                                                              {'field':open_close_sexp, 'names':['Open-1', 'Close-1']},\n",
    "                                                              {'field':up_or_down, 'name': 'Up or down'}]})\n",
    "api.ok(extended_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we confirm that the new dataset has indeed the new columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.take_sample(extended_dataset['resource'], size=3)\n",
    "[{'id':f['id'], 'name':f['name'], 'optype':f['optype']} for f in sampler.fields()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.rows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
